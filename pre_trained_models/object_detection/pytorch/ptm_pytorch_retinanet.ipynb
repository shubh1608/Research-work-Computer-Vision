{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection - Pytorch Retinanet\n",
    "- code reference taken from - https://debuggercafe.com/object-detection-using-retinanet-with-pytorch-and-deep-learning/\n",
    "- https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.detection.retinanet_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import bbox_visualizer as bbv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import time\n",
    "import image_preprocessing_library as lib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Name List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetinaNet(\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelP6P7(\n",
       "        (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (anchor_generator): AnchorGenerator()\n",
       "  (head): RetinaNetHead(\n",
       "    (classification_head): RetinaNetClassificationHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU()\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 819, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (regression_head): RetinaNetRegressionHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU()\n",
       "      )\n",
       "      (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"retinanet_resnet50_fpn\"\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"../../../dataset/object_detection/images\")\n",
    "detection_result_path = Path(\"../experiment_results\")\n",
    "df_columns = [\"image_name\", \"label\", \"detection_score\", \"ymin\", \"xmin\", \"ymax\", \"xmax\", \"model_name\", \"processing_seq_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_seq_dict = {\n",
    "    \"seq_0\" : [], # for raw seq\n",
    "    \"seq_1\" : [\"gray\"],\n",
    "    \"seq_2\" : [\"hsv\"],\n",
    "    \"seq_3\" : [\"sharpen\"],\n",
    "    \"seq_4\" : [\"gray\", \"bilateral_blur\", \"threshold_mean\"],\n",
    "    \"seq_5\" : [\"gray\", \"bilateral_blur\", \"threshold_gaussian\"],\n",
    "    \"seq_6\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\"],\n",
    "    \"seq_7\" : [\"median_blur\"],\n",
    "    \"seq_8\" : [\"gaussian_blur\"],\n",
    "    \"seq_9\" : [\"bilateral_blur\"],\n",
    "    \"seq_10\" : [\"fastnl_blur\"],\n",
    "    \"seq_11\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\", \"opening\"],\n",
    "    \"seq_12\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\", \"closing\"],\n",
    "    \"seq_13\" : [\"opening\"],\n",
    "    \"seq_14\" : [\"closing\"],\n",
    "    \"seq_15\" : [\"gray\", \"sobel\"],\n",
    "    \"seq_16\" : [\"gray\", \"laplacian\"],\n",
    "    \"seq_17\" : [\"gray\", \"canny\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the downloaded models from directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_batch(batch_size, seq_id):\n",
    "    batch_images = []\n",
    "    counter = 1\n",
    "    for image_name in os.listdir(dataset_path):\n",
    "        img = Image.open(dataset_path/image_name)\n",
    "        #TO-DO : pre run this step of converting to RGB and remove from here\n",
    "        rgb_im = img.convert('RGB')\n",
    "        processed_img = apply_cv_transformations(seq_id, rgb_im)\n",
    "        batch_images.append((image_name, processed_img))\n",
    "        if counter % batch_size == 0:\n",
    "            yield batch_images\n",
    "            batch_images = []\n",
    "        counter = counter + 1\n",
    "\n",
    "def apply_cv_transformations(seq_id, pil_img):\n",
    "    cv_img = convert_to_cv_img(pil_img)\n",
    "    operations = get_seq_operations(seq_id)\n",
    "    processed_img = cv_img\n",
    "    for operation in operations:\n",
    "        processed_img = lib.dispatcher[operation](processed_img)\n",
    "    return convert_to_pil_img(processed_img)         \n",
    "\n",
    "def convert_to_pil_img(opencv_img):\n",
    "    if opencv_img.dtype == 'float64':\n",
    "        opencv_img = opencv_img.astype(np.uint8)\n",
    "    if len(opencv_img.shape) > 2:\n",
    "        pil_img = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        pil_img = cv2.cvtColor(opencv_img, cv2.COLOR_GRAY2RGB)\n",
    "    pil_img = Image.fromarray(pil_img)\n",
    "    return pil_img\n",
    "\n",
    "def convert_to_cv_img(pil_img):\n",
    "    np_img_arr = np.asarray(pil_img)\n",
    "    cv_image=cv2.cvtColor(np_img_arr, cv2.COLOR_RGB2BGR)\n",
    "    return cv_image\n",
    "\n",
    "def get_seq_operations(seq_id):\n",
    "    return pre_processing_seq_dict[seq_id]\n",
    "\n",
    "def get_seq_name(seq_id):\n",
    "    if seq_id not in pre_processing_seq_dict.keys():\n",
    "        return \"not found for dataset id: \" + seq_id\n",
    "    return \" > \".join(get_seq_operations(seq_id))\n",
    "\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "def detect_objects(model, pil_images_tuple):\n",
    "    # works on single image\n",
    "    # use Image.fromarray(image_np) for converting below numpy arr img to pil img\n",
    "    detections_list = []\n",
    "    for pil_img_tuple in pil_images_tuple:\n",
    "        tranformed_img = transform(pil_img_tuple[1])\n",
    "        tranformed_img = tranformed_img.unsqueeze(0)\n",
    "        detections = model(tranformed_img)\n",
    "        detections = detections[0] # here taking first element only, as the results is in list form for batch images.\n",
    "        detections_list.append((pil_img_tuple[0], detections))\n",
    "    return detections_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_detections(detections_list, batch_images, denorm = True):\n",
    "    # purpose - filter passed detections based on threshold, and denormalise box coordinates\n",
    "    # input - [(image_name, {boxes, scores, labels, ...})]\n",
    "    # output - [(image_name, {boxes, scores, labels})]\n",
    "    filtered_detection_list = []\n",
    "    for detection_tuple in detections_list:\n",
    "        filtered_dict = {}\n",
    "        image_name = detection_tuple[0]\n",
    "        detections = detection_tuple[1]\n",
    "        \n",
    "        # TO-DO - change logic here to pick the right boxes and labels as per the score\n",
    "        # do note here that the scores are not sorted, hence we may need to take the indexes where the scores is > threshold\n",
    "        # and based on that we can pick the boxes and labels\n",
    "        scores = detections[\"scores\"].tolist()\n",
    "        filtered_indexes = [i for i, e in enumerate(scores) if e > 0.5]\n",
    "        scores = [round(score, 2) for score in scores if score > 0.5]\n",
    "\n",
    "        boxes = detections[\"boxes\"].tolist()\n",
    "        filtered_boxes = [b for i, b in enumerate(boxes) if i in filtered_indexes]\n",
    "        int_boxes = []\n",
    "        for box in filtered_boxes:\n",
    "            box = [int(c) for c in box]\n",
    "            int_boxes.append(box)\n",
    "        \n",
    "        classes = detections[\"labels\"].tolist()\n",
    "        filtered_labels = [l for i, l in enumerate(classes) if i in filtered_indexes]\n",
    "        #filtered_labels = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in filtered_labels]\n",
    "        \n",
    "        filtered_dict[\"detection_scores\"] = scores\n",
    "        filtered_dict[\"detection_boxes\"] = int_boxes\n",
    "        filtered_dict[\"detection_classes\"] = filtered_labels\n",
    "        \n",
    "        filtered_detection_list.append((image_name, filtered_dict))\n",
    "    return filtered_detection_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, images_tuple):\n",
    "    detections_list = detect_objects(model, images_tuple)\n",
    "    # process detections object to output list as per your need\n",
    "    # below method will filter out the boxes based on threshold\n",
    "    detections_list = filter_detections(detections_list, images_tuple)\n",
    "    return detections_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_name(label_id):\n",
    "    if label_id not in list(range(0, len(COCO_INSTANCE_CATEGORY_NAMES))):\n",
    "        return \"NA\"\n",
    "    return COCO_INSTANCE_CATEGORY_NAMES[label_id]\n",
    "\n",
    "def prepare_per_image_res(detection_res):\n",
    "    # detection_res: (image_name, {detection_scores:[], detection_boxes:array(), detection_classes:array()})\n",
    "    num_detections_per_image = len(detection_res[1][\"detection_boxes\"])\n",
    "    rows = []\n",
    "    for i in range(0, num_detections_per_image):\n",
    "        image_name = detection_res[0]\n",
    "        label_id = detection_res[1][\"detection_classes\"][i]\n",
    "        label = get_label_name(label_id)\n",
    "        box = detection_res[1][\"detection_boxes\"][i]\n",
    "        score = round(detection_res[1][\"detection_scores\"][i], 2)\n",
    "        # retinanet gives results in format xmin, ymin, xmax, ymax\n",
    "        # and we store in the format ymin, xmin, ymax, xmax\n",
    "        row = [image_name, label, score, int(box[1]), int(box[0]), int(box[3]), int(box[2])]\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def store_results(detection_results, pre_processing_seq_name):\n",
    "    # process detection_results for storing it in the df/csv\n",
    "    #     round of the bbox coordinates\n",
    "    df_rows = []\n",
    "    for res in detection_results:\n",
    "        rows = prepare_per_image_res(res)\n",
    "        df_rows.extend(rows)\n",
    "            \n",
    "    # append model_name, seq_name at the end of rows\n",
    "    info = [model_name, pre_processing_seq_name]\n",
    "    for row in df_rows:\n",
    "        row.extend(info)\n",
    "    \n",
    "    # create a df for storing the results\n",
    "    df = pd.DataFrame(df_rows, columns = df_columns)\n",
    "    \n",
    "    # check if csv file exists, if yes then append result, if not the create and dump the result\n",
    "    if not os.path.exists(detection_result_path/(model_name + \".csv\")):\n",
    "        # store the df in to model_name.csv file\n",
    "        df.to_csv(detection_result_path/(model_name + \".csv\"), index=False)\n",
    "    else:\n",
    "        stored_df = pd.read_csv(detection_result_path/(model_name + \".csv\"))\n",
    "        stored_df = stored_df.append(df)\n",
    "        stored_df.to_csv(detection_result_path/(model_name + \".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN below for all inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_id in pre_processing_seq_dict.keys():\n",
    "    print(\"processing for seq_id:{0} started.\".format(seq_id))\n",
    "    counter = 1\n",
    "    for batch_images in load_images_batch(50, seq_id):\n",
    "        print(\"{0}\".format(counter), end=\"...\")\n",
    "        detection_result = run_inference(model, batch_images)\n",
    "        store_results(detection_result, seq_id)\n",
    "        counter = counter + 1\n",
    "    print(\"----------------\")\n",
    "print(\"Inference completed for model:{0}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_images = next(load_images_batch(2, \"seq_1\"))\n",
    "# res = run_inference(model, batch_images)\n",
    "# store_results(res, \"seq_0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
