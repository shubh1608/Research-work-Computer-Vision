{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch - Pre Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import json\n",
    "import image_preprocessing_library as lib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "#     \"resnet18\" : models.resnet18(pretrained=True),\n",
    "#     \"alexnet\" : models.alexnet(pretrained=True),\n",
    "#     \"squeezenet\" : models.squeezenet1_0(pretrained=True),\n",
    "#     \"vgg16\" : models.vgg16(pretrained=True),\n",
    "#     \"densenet161\" : models.densenet161(pretrained=True),\n",
    "#     \"googlenet\" : models.googlenet(pretrained=True),\n",
    "#     \"shufflenet_v2\" : models.shufflenet_v2_x1_0(pretrained=True),\n",
    "#     \"mobilenet_v2\" : models.mobilenet_v2(pretrained=True),\n",
    "#     \"resnext50_32x4d\" : models.resnext50_32x4d(pretrained=True),\n",
    "    \"wide_resnet50_2\" : models.wide_resnet50_2(pretrained=True),\n",
    "    \"mnasnet\" : models.mnasnet1_0(pretrained=True),\n",
    "    \"inception_v3\" : models.inception_v3(pretrained=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_seq_dict = {\n",
    "    \"seq_0\" : [], # for raw seq\n",
    "    \"seq_1\" : [\"gray\"],\n",
    "    \"seq_2\" : [\"hsv\"],\n",
    "    \"seq_3\" : [\"sharpen\"],\n",
    "    \"seq_4\" : [\"gray\", \"bilateral_blur\", \"threshold_mean\"],\n",
    "    \"seq_5\" : [\"gray\", \"bilateral_blur\", \"threshold_gaussian\"],\n",
    "    \"seq_6\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\"],\n",
    "    \"seq_7\" : [\"median_blur\"],\n",
    "    \"seq_8\" : [\"gaussian_blur\"],\n",
    "    \"seq_9\" : [\"bilateral_blur\"],\n",
    "    \"seq_10\" : [\"fastnl_blur\"],\n",
    "    \"seq_11\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\", \"opening\"],\n",
    "    \"seq_12\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\", \"closing\"],\n",
    "    \"seq_13\" : [\"opening\"],\n",
    "    \"seq_14\" : [\"closing\"],\n",
    "    \"seq_15\" : [\"gray\", \"sobel\"],\n",
    "    \"seq_16\" : [\"gray\", \"laplacian\"],\n",
    "    \"seq_17\" : [\"gray\", \"canny\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config/Map files load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary containing imagenet - custom label maps\n",
    "imagenet_label_map = None\n",
    "with open('./config_jsons/imagenet_label_map.json') as json_file: \n",
    "    imagenet_label_map = json.load(json_file)\n",
    "\n",
    "# dictionary containing model ids - model_name map\n",
    "model_ids = None\n",
    "with open('./config_jsons/model_ids_map.json') as json_file: \n",
    "    model_ids = json.load(json_file)\n",
    "\n",
    "# dictionary containing dataset_ids and dataset_desc    \n",
    "dataset_ids = None\n",
    "with open('../../dataset/image_classification/dataset_id_map.json') as json_file:\n",
    "    dataset_ids = json.load(json_file)\n",
    "    \n",
    "custom_dataset_labels = None\n",
    "with open('./config_jsons/label_ids_map.json') as json_file:\n",
    "    custom_dataset_labels = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change in approach, instead of generating n dataset, we will process it on memory, i.e process images while evaluating.\n",
    "Consider making below cell parameterised later using papermill, or you may prefer to do it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking path as a parameter here, to ease out refactoring in future.\n",
    "# first experimentation is on raw dataset, will scale out to others\n",
    "dataset_path = \"../../dataset/image_classification/raw\"\n",
    "master_df_path = \"./experiment_results/final_results/master_pytorch.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and declarations\n",
    "base_path = Path(dataset_path)\n",
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
    "response = requests.get(LABELS_URL)  \n",
    "labels = {int(key): value for key, value in response.json().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master dataframe initialise\n",
    "# create a dataframe for storing per image predictions\n",
    "# columns = [\"model_id\", \"model_name\", \"seq_id\", \"seq_name\", \"image_name\", \"label_id\", \n",
    "#             \"pred_label_id\", \"label_name\",\"pred_label_name\", \"pred_confidence\"]\n",
    "# temp = pd.DataFrame(columns = columns)\n",
    "# temp.to_csv(master_df_path, index=False)\n",
    "\n",
    "# above code is to create a master empty csv file with our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"model_id\", \"model_name\", \"seq_id\", \"seq_name\", \"image_name\", \"label_id\", \n",
    "             \"pred_label_id\", \"label_name\",\"pred_label_name\", \"pred_confidence\"]\n",
    "\n",
    "# master_df = pd.read_csv('./experiment_results/master.csv')\n",
    "\n",
    "# # creating a current run dataframe, for efficiency\n",
    "# current_run_df = pd.DataFrame(columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_labels = [x for x in os.walk(base_path)][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenCV functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pil_img(opencv_img):\n",
    "    if opencv_img.dtype == 'float64':\n",
    "        opencv_img = opencv_img.astype(np.uint8)\n",
    "    pil_img = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(pil_img)\n",
    "    return pil_img\n",
    "\n",
    "def convert_to_cv_img(pil_img):\n",
    "    np_img_arr = np.asarray(pil_img)\n",
    "    cv_image=cv2.cvtColor(np_img_arr, cv2.COLOR_RGB2BGR)\n",
    "    return cv_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id(model_name):\n",
    "    for id, name in model_ids.items():\n",
    "        if name == model_name:\n",
    "            return id\n",
    "    return \"not found for model name: \" + model_name\n",
    "\n",
    "def get_model_name(id):\n",
    "    if id not in model_ids.keys():\n",
    "        return \"not found for model id: \" + str(id)\n",
    "    return model_ids[id]\n",
    "\n",
    "def get_seq_name(seq_id):\n",
    "    if seq_id not in pre_processing_seq_dict.keys():\n",
    "        return \"not found for dataset id: \" + seq_id\n",
    "    return \" > \".join(get_seq_operations(seq_id))\n",
    "\n",
    "def get_label_id(label_name):\n",
    "    for id, name in custom_dataset_labels.items():\n",
    "        if name == label_name:\n",
    "            return id\n",
    "    return \"not found for label: \"+label_name\n",
    "\n",
    "def get_label_name(id):\n",
    "    if id not in custom_dataset_labels.keys():\n",
    "        return \"not found for id: \" + str(id)\n",
    "    return custom_dataset_labels[id]\n",
    "\n",
    "def get_pred_label_id(pred_label_name):\n",
    "    for custom_label, imagenet_labels in imagenet_label_map.items():\n",
    "        if pred_label_name.lower() in imagenet_labels:\n",
    "            return get_label_id(custom_label)\n",
    "    return get_label_id(\"others\")\n",
    "\n",
    "def get_seq_operations(seq_id):\n",
    "    return pre_processing_seq_dict[seq_id]\n",
    "\n",
    "def load_master_df_from_csv():\n",
    "    return pd.read_csv(master_df_path)\n",
    "\n",
    "def dump_master_df_to_csv(master_df):\n",
    "    master_df.to_csv(master_df_path, header=True, columns=columns, index=False)\n",
    "    \n",
    "def log_per_label_results_to_csv(model_name, label_name, pred_result_list, seq_id):\n",
    "    # input - pred_result_list: (label, pred_tuple_list), pred_tuple_list: [(image_name, pred_label, pred_percentage)]\n",
    "    # columns = [\"model_id\", \"model_name\", \"seq_id\", \"seq_name\", \"image_name\", \"label_id\", \n",
    "    #            \"pred_label_id\", \"label_name\",\"pred_label_name\", \"pred_confidence\"]\n",
    "\n",
    "    model_id = get_model_id(model_name)\n",
    "    model_name = model_name\n",
    "    # change below before running on pre processed image dataset\n",
    "    seq_id = seq_id\n",
    "    seq_name = get_seq_name(seq_id)\n",
    "    label_id = get_label_id(label_name)\n",
    "    label_name = label_name\n",
    "    \n",
    "    rows = []\n",
    "    for res in pred_result_list:\n",
    "        image_name = res[0]\n",
    "        pred_label_name = res[1]\n",
    "        pred_confidence = res[2]\n",
    "        pred_label_id = get_pred_label_id(pred_label_name)\n",
    "        row = [model_id, model_name, seq_id, seq_name, image_name, label_id, label_name, \n",
    "               pred_label_id, pred_label_name, pred_confidence]\n",
    "        rows.append(row)\n",
    "    \n",
    "    current_label_df = pd.DataFrame(rows, columns = columns)\n",
    "    master_df = load_master_df_from_csv()\n",
    "    master_df = master_df.append(current_label_df)\n",
    "    dump_master_df_to_csv(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_gen = next(load_per_label_imgs_generator())\n",
    "# trans_img = apply_tranformations(img_gen[1][0][1], \"seq_15\")\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# model.eval()\n",
    "# out = model(trans_img)\n",
    "# _, index = torch.max(out, 1)\n",
    "# percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "# print(\"label is: \" + labels[index[0].item()] + \" and perc is: \" + str(percentage[index[0]].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it works on list of images and return a list\n",
    "def load_per_label_imgs_generator():  \n",
    "    #better due to memory restrictions we read per class images at once i.e airplane folder at a time.\n",
    "    # output: (label, [(image_name, pil_image)])\n",
    "    \n",
    "    # consider removing \"others\" label id from custom_dataset_label, while reading based on its values.\n",
    "    for label in custom_labels:\n",
    "        per_label_images = []\n",
    "        label_image_names = [x for x in os.walk(base_path/label)][0][2]\n",
    "        for image_name in label_image_names:\n",
    "            img = Image.open(base_path/label/image_name)\n",
    "            #TO-DO : pre run this step of converting to RGB and remove from here\n",
    "            rgb_im = img.convert('RGB')\n",
    "            per_label_images.append((image_name, rgb_im))\n",
    "        # change below line to read all images, currently it will read only 1 image per label\n",
    "        yield (label, per_label_images)\n",
    "\n",
    "    \n",
    "def apply_cv_transformations(seq_id, pil_img):\n",
    "    cv_img = convert_to_cv_img(pil_img)\n",
    "    operations = get_seq_operations(seq_id)\n",
    "    processed_img = cv_img\n",
    "    for operation in operations:\n",
    "        processed_img = lib.dispatcher[operation](processed_img)\n",
    "    return convert_to_pil_img(processed_img)\n",
    "\n",
    "\n",
    "def apply_tranformations(img, seq_id):\n",
    "    isGrayImgs = \"gray\" in get_seq_operations(seq_id)\n",
    "    # transform dict for color and gray\n",
    "    transform_dict = {\n",
    "        \"resize\" : transforms.Compose([            \n",
    "            transforms.Resize(256),                    \n",
    "            transforms.CenterCrop(224)               \n",
    "        ]),\n",
    "        \"color\" : transforms.Compose([              \n",
    "            transforms.ToTensor(),                     \n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        \"gray\" : transforms.Compose([              \n",
    "            transforms.ToTensor()                \n",
    "        ])\n",
    "    }\n",
    "    # resizing using torch transform\n",
    "    resized_img = transform_dict[\"resize\"](img)\n",
    "\n",
    "    # applying cv transforms\n",
    "    transformed_img = apply_cv_transformations(seq_id, resized_img)\n",
    "\n",
    "    # choosing torch tranform\n",
    "    if isGrayImgs :\n",
    "        transformed_img = transform_dict[\"gray\"](transformed_img)\n",
    "    else:\n",
    "        transformed_img = transform_dict[\"color\"](transformed_img)\n",
    "\n",
    "    #convert it in to format(batch_size, channel, height, width)\n",
    "    transformed_img = transformed_img.unsqueeze(0)\n",
    "    return Variable(transformed_img)\n",
    "\n",
    "# works on list of images and returns a list\n",
    "def transform_images(imgs, seq_id):    \n",
    "    #input: imgs is [(image_name, pil_image)]\n",
    "    #output: [(imagename, pil_transformed_image)]\n",
    "    results = []\n",
    "    \n",
    "    for img_tuple in imgs:\n",
    "        results.append((img_tuple[0], apply_tranformations(img_tuple[1], seq_id)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(model, batch_imgs):  \n",
    "    #input: model is pytorch model, batch_imgs is [(image_name, pil_transformed_image)]\n",
    "    #output: results is [(image_name, pred_label, pred_percentage)]\n",
    "    \n",
    "    results = []\n",
    "    for batch_img in batch_imgs:  \n",
    "        model.eval()\n",
    "        out = model(batch_img[1])\n",
    "        _, index = torch.max(out, 1)\n",
    "        percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "        results.append((batch_img[0], labels[index[0].item()], percentage[index[0]].item()))\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def run_per_seq_model_inference(model, model_name, seq_id):\n",
    "    # input model: any pytorch pre trained model\n",
    "    # per_label_images_tuple: currently it assumers per label images, and of the form (label, [label_images])\n",
    "    # output: (label, pred_tuple_list), pred_tuple_list: [(image_name, pred_label, pred_percentage)]\n",
    "    \n",
    "    # load images here, and take pre-processing sequence as an input\n",
    "    for per_label_images_tuple in load_per_label_imgs_generator():\n",
    "        tranformed_imgs = transform_images(per_label_images_tuple[1], seq_id)\n",
    "        # logging to csv is done inside below function, hence need to pass parameters\n",
    "        pred_res = evaluate_results(model, tranformed_imgs) \n",
    "        label_name = per_label_images_tuple[0]\n",
    "        log_per_label_results_to_csv(model_name, label_name, pred_res, seq_id)\n",
    "\n",
    "    \n",
    "def run_all_model_inference(model_dict):\n",
    "    for model_name, model in model_dict.items():\n",
    "        print(\"Started inference for: \"+model_name)\n",
    "        for seq_id in pre_processing_seq_dict.keys():\n",
    "            print(\"inference started on: \" + str(seq_id) + \", \", end='')\n",
    "            run_per_seq_model_inference(model, model_name, seq_id)\n",
    "            print(\"______ completed!\")\n",
    "        print(\"completed inference -----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started inference for: wide_resnet50_2\n",
      "inference started on: seq_0, ______ completed!\n",
      "inference started on: seq_1, ______ completed!\n",
      "inference started on: seq_2, ______ completed!\n",
      "inference started on: seq_3, ______ completed!\n",
      "inference started on: seq_4, ______ completed!\n",
      "inference started on: seq_5, ______ completed!\n",
      "inference started on: seq_6, ______ completed!\n",
      "inference started on: seq_7, ______ completed!\n",
      "inference started on: seq_8, ______ completed!\n",
      "inference started on: seq_9, ______ completed!\n",
      "inference started on: seq_10, ______ completed!\n",
      "inference started on: seq_11, ______ completed!\n",
      "inference started on: seq_12, ______ completed!\n",
      "inference started on: seq_13, ______ completed!\n",
      "inference started on: seq_14, ______ completed!\n",
      "inference started on: seq_15, ______ completed!\n",
      "inference started on: seq_16, ______ completed!\n",
      "inference started on: seq_17, ______ completed!\n",
      "completed inference -----------------------------------\n",
      "Started inference for: mnasnet\n",
      "inference started on: seq_0, ______ completed!\n",
      "inference started on: seq_1, ______ completed!\n",
      "inference started on: seq_2, ______ completed!\n",
      "inference started on: seq_3, ______ completed!\n",
      "inference started on: seq_4, ______ completed!\n",
      "inference started on: seq_5, ______ completed!\n",
      "inference started on: seq_6, ______ completed!\n",
      "inference started on: seq_7, ______ completed!\n",
      "inference started on: seq_8, ______ completed!\n",
      "inference started on: seq_9, ______ completed!\n",
      "inference started on: seq_10, ______ completed!\n",
      "inference started on: seq_11, ______ completed!\n",
      "inference started on: seq_12, ______ completed!\n",
      "inference started on: seq_13, ______ completed!\n",
      "inference started on: seq_14, ______ completed!\n",
      "inference started on: seq_15, ______ completed!\n",
      "inference started on: seq_16, ______ completed!\n",
      "inference started on: seq_17, ______ completed!\n",
      "completed inference -----------------------------------\n",
      "Started inference for: inception_v3\n",
      "inference started on: seq_0, ______ completed!\n",
      "inference started on: seq_1, ______ completed!\n",
      "inference started on: seq_2, ______ completed!\n",
      "inference started on: seq_3, ______ completed!\n",
      "inference started on: seq_4, ______ completed!\n",
      "inference started on: seq_5, ______ completed!\n",
      "inference started on: seq_6, ______ completed!\n",
      "inference started on: seq_7, ______ completed!\n",
      "inference started on: seq_8, ______ completed!\n",
      "inference started on: seq_9, ______ completed!\n",
      "inference started on: seq_10, ______ completed!\n",
      "inference started on: seq_11, ______ completed!\n",
      "inference started on: seq_12, ______ completed!\n",
      "inference started on: seq_13, ______ completed!\n",
      "inference started on: seq_14, ______ completed!\n",
      "inference started on: seq_15, ______ completed!\n",
      "inference started on: seq_16, ______ completed!\n",
      "inference started on: seq_17, ______ completed!\n",
      "completed inference -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "run_all_model_inference(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
