{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS - EfficientNet Models\n",
    "* https://github.com/qubvel/efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import requests\n",
    "import sys, os\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import image_preprocessing_library as lib\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from efficientnet.keras import EfficientNetB0\n",
    "from efficientnet.keras import EfficientNetB1\n",
    "from efficientnet.keras import EfficientNetB2\n",
    "from efficientnet.keras import EfficientNetB3\n",
    "from efficientnet.keras import EfficientNetB4\n",
    "from efficientnet.keras import EfficientNetB5\n",
    "from efficientnet.keras import EfficientNetB6\n",
    "from efficientnet.keras import EfficientNetB7\n",
    "from efficientnet.keras import center_crop_and_resize, preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:Large dropout rate: 0.5125 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.525 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.5375 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.55 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.5625 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading pretrained model\n",
    "efn_models = {\n",
    "#     \"efficientnetb0\" :  EfficientNetB0(weights='imagenet'),\n",
    "#     \"efficientnetb1\" :  EfficientNetB1(weights='imagenet'),\n",
    "#     \"efficientnetb2\" :  EfficientNetB2(weights='imagenet'),\n",
    "#     \"efficientnetb3\" :  EfficientNetB3(weights='imagenet'),\n",
    "#     \"efficientnetb4\" :  EfficientNetB4(weights='imagenet'),\n",
    "#     \"efficientnetb5\" :  EfficientNetB5(weights='imagenet'),\n",
    "#     \"efficientnetb6\" :  EfficientNetB6(weights='imagenet'),\n",
    "    \"efficientnetb7\" :  EfficientNetB7(weights='imagenet')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_seq_dict = {\n",
    "#     \"seq_0\" : [], # for raw seq\n",
    "#     \"seq_1\" : [\"gray\"],\n",
    "#     \"seq_2\" : [\"hsv\"],\n",
    "#     \"seq_3\" : [\"sharpen\"],\n",
    "#    \"seq_4\" : [\"gray\", \"bilateral_blur\", \"threshold_mean\"],\n",
    "    \"seq_5\" : [\"gray\", \"bilateral_blur\", \"threshold_gaussian\"],\n",
    "    \"seq_6\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\"],\n",
    "#     \"seq_7\" : [\"median_blur\"],\n",
    "#     \"seq_8\" : [\"gaussian_blur\"],\n",
    "#     \"seq_9\" : [\"bilateral_blur\"],\n",
    "#     \"seq_10\" : [\"fastnl_blur\"],\n",
    "#     \"seq_11\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\", \"opening\"],\n",
    "#     \"seq_12\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\", \"closing\"],\n",
    "#     \"seq_13\" : [\"opening\"],\n",
    "#     \"seq_14\" : [\"closing\"],\n",
    "#     \"seq_15\" : [\"gray\", \"sobel\"],\n",
    "#     \"seq_16\" : [\"gray\", \"laplacian\"],\n",
    "#     \"seq_17\" : [\"gray\", \"canny\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config/Map Files Load\n",
    "* change below paths while running on GPU\n",
    "* you may prefer to dump config files/jsons in the same notebook dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary containing imagenet - custom label maps\n",
    "imagenet_label_map = None\n",
    "with open('./config_jsons/imagenet_label_map.json') as json_file: \n",
    "    imagenet_label_map = json.load(json_file)\n",
    "\n",
    "# dictionary containing model ids - model_name map\n",
    "model_ids = None\n",
    "with open('./config_jsons/model_ids_map.json') as json_file: \n",
    "    model_ids = json.load(json_file)\n",
    "\n",
    "# dictionary containing dataset_ids and dataset_desc    \n",
    "dataset_ids = None\n",
    "with open('../../dataset/image_classification/dataset_id_map.json') as json_file:\n",
    "    dataset_ids = json.load(json_file)\n",
    "    \n",
    "custom_dataset_labels = None\n",
    "with open('./config_jsons/label_ids_map.json') as json_file:\n",
    "    custom_dataset_labels = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider making below cell parameterised later using papermill, or you may prefer to do it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking path as a parameter here, to ease out refactoring in future.\n",
    "# first experimentation is on raw dataset, will scale out to others\n",
    "dataset_path = Path(\"../../dataset/image_classification/raw\")\n",
    "master_df_keras_path = \"./experiment_results/final_results/master_keras_efficientnet.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe columns\n",
    "columns = [\"model_id\", \"model_name\", \"seq_id\", \"seq_name\", \"image_name\", \"label_id\", \n",
    "             \"pred_label_id\", \"label_name\",\"pred_label_name\", \"pred_confidence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(columns = columns)\n",
    "# df.to_csv(master_df_keras_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_labels = [x for x in os.walk(dataset_path)][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenCV to PIL image conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pil_img(opencv_img):\n",
    "    if opencv_img.dtype == 'float64':\n",
    "        opencv_img = opencv_img.astype(np.uint8)\n",
    "    pil_img = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(pil_img)\n",
    "    return pil_img\n",
    "\n",
    "def convert_to_cv_img(pil_img):\n",
    "    np_img_arr = np.asarray(pil_img)\n",
    "    cv_image=cv2.cvtColor(np_img_arr, cv2.COLOR_RGB2BGR)\n",
    "    return cv_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id(model_name):\n",
    "    for id, name in model_ids.items():\n",
    "        if name == model_name:\n",
    "            return id\n",
    "    return \"not found for model name: \" + model_name\n",
    "\n",
    "def get_model_name(id):\n",
    "    if id not in model_ids.keys():\n",
    "        return \"not found for model id: \" + str(id)\n",
    "    return model_ids[id]\n",
    "\n",
    "def get_seq_operations(seq_id):\n",
    "    return pre_processing_seq_dict[seq_id]\n",
    "\n",
    "def get_seq_name(seq_id):\n",
    "    if seq_id not in pre_processing_seq_dict.keys():\n",
    "        return \"not found for dataset id: \" + seq_id\n",
    "    return \" > \".join(get_seq_operations(seq_id))\n",
    "\n",
    "def get_label_id(label_name):\n",
    "    for id, name in custom_dataset_labels.items():\n",
    "        if name == label_name:\n",
    "            return id\n",
    "    return \"not found for label: \"+label_name\n",
    "\n",
    "def get_label_name(id):\n",
    "    if id not in custom_dataset_labels.keys():\n",
    "        return \"not found for id: \" + str(id)\n",
    "    return custom_dataset_labels[id]\n",
    "\n",
    "def get_pred_label_id(pred_label_name):\n",
    "    for custom_label, imagenet_labels in imagenet_label_map.items():\n",
    "        if pred_label_name.lower() in imagenet_labels:\n",
    "            return get_label_id(custom_label)\n",
    "    return get_label_id(\"others\")\n",
    "\n",
    "def load_master_df_from_csv():\n",
    "    return pd.read_csv(master_df_keras_path)\n",
    "\n",
    "def dump_master_df_to_csv(master_df):\n",
    "    master_df.to_csv(master_df_keras_path, header=True, columns=columns, index=False)       \n",
    "\n",
    "def log_per_label_results_to_csv(model_name, label_name, pred_result_list, seq_id):\n",
    "    # input - pred_result_list: (label, pred_tuple_list), pred_tuple_list: [(image_name, pred_label, pred_percentage)]\n",
    "    # columns = [\"model_id\", \"model_name\", \"dataset_id\", \"dataset_name\", \"image_name\", \"label_id\", \n",
    "    #            \"pred_label_id\", \"label_name\",\"pred_label_name\", \"pred_confidence\"]\n",
    "\n",
    "    model_id = get_model_id(model_name)\n",
    "    model_name = model_name\n",
    "    # change below before running on pre processed image dataset\n",
    "    seq_id = seq_id\n",
    "    seq_name = get_seq_name(seq_id)\n",
    "    label_id = get_label_id(label_name)\n",
    "    label_name = label_name\n",
    "    \n",
    "    rows = []\n",
    "    for res in pred_result_list:\n",
    "        image_name = res[0]\n",
    "        pred_label_name = res[1]\n",
    "        pred_confidence = res[2]\n",
    "        pred_label_id = get_pred_label_id(pred_label_name)\n",
    "        row = [model_id, model_name, seq_id, seq_name, image_name, label_id, label_name, \n",
    "               pred_label_id, pred_label_name, pred_confidence]\n",
    "        rows.append(row)\n",
    "    \n",
    "    current_label_df = pd.DataFrame(rows, columns = columns)\n",
    "    master_df = load_master_df_from_csv()\n",
    "    master_df = master_df.append(current_label_df)\n",
    "    dump_master_df_to_csv(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_per_label_imgs_generator(seq_id):\n",
    "    # better due to memory restrictions we read per class images at once i.e airplane folder at a time.\n",
    "    # output: (label, [(image_name, pil_image)])\n",
    "    for label in custom_labels:\n",
    "        per_label_images = []\n",
    "        label_image_names = [x for x in os.walk(dataset_path/label)][0][2]\n",
    "        for image_name in label_image_names:\n",
    "            img_path = dataset_path/label/image_name\n",
    "            # reading pil image below\n",
    "            img = Image.open(img_path)\n",
    "            img = apply_cv_transformations(seq_id, img)\n",
    "            x = image.img_to_array(img)\n",
    "            #x = np.expand_dims(x, axis=0)\n",
    "            per_label_images.append((image_name, x))\n",
    "        yield (label, per_label_images)\n",
    "\n",
    "def apply_cv_transformations(seq_id, pil_img):\n",
    "    cv_img = convert_to_cv_img(pil_img)\n",
    "    operations = get_seq_operations(seq_id)\n",
    "    processed_img = cv_img\n",
    "    for operation in operations:\n",
    "        processed_img = lib.dispatcher[operation](processed_img)\n",
    "    return convert_to_pil_img(processed_img) \n",
    "    \n",
    "        \n",
    "def transform_images(imgs_tuple, model):\n",
    "    # input: imgs_tuple: (image_name, pil_image)\n",
    "    # output: processed_imgs: (image_name, processed_image)\n",
    "    processed_imgs = []\n",
    "    image_size = model.input_shape[1]\n",
    "    for img_tuple in imgs_tuple:\n",
    "        x = center_crop_and_resize(img_tuple[1], image_size=image_size)\n",
    "        x = preprocess_input(x)\n",
    "        x = np.expand_dims(x, 0)\n",
    "        processed_imgs.append((img_tuple[0],x))\n",
    "    return processed_imgs\n",
    "\n",
    "\n",
    "def evaluate_results(model, imgs):\n",
    "    # input: imgs = (img_name, pil_image)\n",
    "    # output: results = [(img_name, pred_label, prob)]\n",
    "    results = []\n",
    "    for img in imgs:\n",
    "        pred = model.predict(img[1])\n",
    "        (id, label, label_prob) = decode_predictions(pred, top=1)[0][0]\n",
    "        results.append((img[0], label, label_prob))\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_per_label_model_inference(model, model_name, seq_id):\n",
    "    # input - model: which model?, preprocess_method: model dependent method, decode_predictions: model dependent\n",
    "    # input - default_size = true, means load image in 299x299 or 224x224?\n",
    "    # imgs_tuple: (label, [images])\n",
    "    # output - (label, pred_tuple_list), here pred_tuple_list: [(image_name, pred_label, prob)]\n",
    "    for imgs_tuple in load_per_label_imgs_generator(seq_id):\n",
    "        label_name = imgs_tuple[0]\n",
    "        if seq_id == \"seq_5\" and label_name in [\"airplane\", \"apple\", \"backpack\", \"banana\", \"bathtub\"]:\n",
    "            continue\n",
    "        processed_imgs = transform_images(imgs_tuple[1], model)\n",
    "        pred_res = evaluate_results(model, processed_imgs)\n",
    "        log_per_label_results_to_csv(model_name, label_name, pred_res, seq_id)\n",
    "    \n",
    "def model_generator():\n",
    "    for model_name in efn_models.keys():\n",
    "        yield (model_name, efn_models[model_name])    \n",
    "\n",
    "def run_all_model_inference():\n",
    "    for model_tuple in model_generator():\n",
    "        #model_tuple = next(model_generator())\n",
    "        model_name = model_tuple[0]\n",
    "        model = model_tuple[1]\n",
    "        print(\"Inference started for model: \" + model_name)\n",
    "        for seq_id in pre_processing_seq_dict.keys():\n",
    "            print(\"inference started for seq_id: \" + seq_id + \", \", end='')\n",
    "            run_per_label_model_inference(model, model_name, seq_id)\n",
    "            print(\"completed!\")\n",
    "        print(\"Inference completed! -----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference started for model: efficientnetb7\n",
      "inference started for seq_id: seq_5, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6433139424a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# need to change the save master_df path and assign model ids as well\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrun_all_model_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-33f717895e55>\u001b[0m in \u001b[0;36mrun_all_model_inference\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mseq_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpre_processing_seq_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inference started for seq_id: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mseq_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mrun_per_label_model_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"completed!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Inference completed! -----------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-33f717895e55>\u001b[0m in \u001b[0;36mrun_per_label_model_inference\u001b[1;34m(model, model_name, seq_id)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mprocessed_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mpred_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessed_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mlog_per_label_results_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-33f717895e55>\u001b[0m in \u001b[0;36mevaluate_results\u001b[1;34m(model, imgs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_prob\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1401\u001b[1;33m                                             callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# need to change the save master_df path and assign model ids as well\n",
    "run_all_model_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference started for model: efficientnetb7<br>\n",
    "inference started for seq_id: seq_0, completed!<br>\n",
    "inference started for seq_id: seq_1, completed!<br>\n",
    "inference started for seq_id: seq_2, completed!<br>\n",
    "inference started for seq_id: seq_3, completed!<br>\n",
    "inference started for seq_id: seq_4, completed!<br>\n",
    "inference started for seq_id: seq_5,  <b><i>I STOPPED EXECUTION HERE! thoda aaraam bhi do yaar!</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re Run this from seq_4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
