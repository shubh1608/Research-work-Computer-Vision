{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS - Pre Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.applications.xception import Xception\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg19 import VGG19\n",
    "# from keras.applications.resnet import ResNet50\n",
    "# from keras.applications.resnet import ResNet101\n",
    "# from keras.applications.resnet import ResNet152\n",
    "# from keras.applications.resnet_v2 import ResNet50V2\n",
    "# from keras.applications.resnet_v2 import ResNet101V2\n",
    "# from keras.applications.resnet_v2 import ResNet152V2\n",
    "# from keras.applications.resnext import ResNeXt50\n",
    "# from keras.applications.resnext import ResNeXt101\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "# from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "# from keras.applications.mobilenet import MobileNet\n",
    "# from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "# from keras.applications.densenet import DenseNet121\n",
    "# from keras.applications.densenet import DenseNet169\n",
    "# from keras.applications.densenet import DenseNet201\n",
    "# from keras.applications.nasnet import NASNetLarge\n",
    "# from keras.applications.nasnet import NASNetMobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import requests\n",
    "import sys, os\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import image_preprocessing_library as lib\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# appending common pakcage in the sys.path for module to be found during imports\n",
    "# add this line to other files as well, IMP! it is. \n",
    "#sys.path.append(os.path.join(os.path.dirname(sys.path[0]),'..','common'))\n",
    "#from common import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptms = {}\n",
    "\n",
    "from keras.applications.resnet import ResNet50\n",
    "from keras.applications.resnet import preprocess_input as resnet50_preprocess_input\n",
    "from keras.applications.resnet import decode_predictions as resnet50_decode_predictions\n",
    "def get_resnet50():\n",
    "    return (ResNet50(weights='imagenet'), resnet50_preprocess_input, resnet50_decode_predictions)\n",
    "ptms[\"resnet50\"] = get_resnet50\n",
    "\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input as vgg19_preprocess_input\n",
    "from keras.applications.vgg19 import decode_predictions as vgg19_decode_predictions\n",
    "def get_vgg19():\n",
    "    return (VGG19(weights='imagenet'), vgg19_preprocess_input, vgg19_decode_predictions)\n",
    "ptms[\"vgg19\"] = get_vgg19\n",
    "\n",
    "\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input as xception_preprocess_input\n",
    "from keras.applications.xception import decode_predictions as xception_decode_predictions\n",
    "def get_xception():\n",
    "    return (Xception(weights='imagenet'), xception_preprocess_input, xception_decode_predictions)\n",
    "ptms[\"xception\"] = get_xception\n",
    "\n",
    "from keras.applications.resnet import ResNet101\n",
    "from keras.applications.resnet import preprocess_input as resnet101_preprocess_input\n",
    "from keras.applications.resnet import decode_predictions as resnet101_decode_predictions\n",
    "def get_resnet101():\n",
    "    return (ResNet101(weights='imagenet'), resnet101_preprocess_input, resnet101_decode_predictions)\n",
    "ptms[\"resnet101\"] = get_resnet101\n",
    "\n",
    "from keras.applications.resnet import ResNet152\n",
    "from keras.applications.resnet import preprocess_input as resnet152_preprocess_input\n",
    "from keras.applications.resnet import decode_predictions as resnet152_decode_predictions\n",
    "def get_resnet152():\n",
    "    return (ResNet152(weights='imagenet'), resnet152_preprocess_input, resnet152_decode_predictions)\n",
    "ptms[\"resnet152\"] = get_resnet152\n",
    "\n",
    "from keras.applications.resnet_v2 import ResNet50V2\n",
    "from keras.applications.resnet_v2 import preprocess_input as resnet50v2_preprocess_input\n",
    "from keras.applications.resnet_v2 import decode_predictions as resnet50v2_decode_predictions\n",
    "def get_resnet50v2():\n",
    "    return (ResNet50V2(weights='imagenet'), resnet50v2_preprocess_input, resnet50v2_decode_predictions)\n",
    "ptms[\"resnet50v2\"] = get_resnet50v2\n",
    "\n",
    "from keras.applications.resnet_v2 import ResNet101V2\n",
    "from keras.applications.resnet_v2 import preprocess_input as resnet101v2_preprocess_input\n",
    "from keras.applications.resnet_v2 import decode_predictions as resnet101v2_decode_predictions\n",
    "def get_resnet101v2():\n",
    "    return (ResNet101V2(weights='imagenet'), resnet101v2_preprocess_input, resnet101v2_decode_predictions)\n",
    "ptms[\"resnet101v2\"] = get_resnet101v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing_seq_dict = {\n",
    "    \"seq_0\" : [], # for raw seq\n",
    "    \"seq_1\" : [\"gray\"],\n",
    "    \"seq_2\" : [\"hsv\"],\n",
    "    \"seq_3\" : [\"sharpen\"],\n",
    "    \"seq_4\" : [\"gray\", \"bilateral_blur\", \"threshold_mean\"],\n",
    "    \"seq_5\" : [\"gray\", \"bilateral_blur\", \"threshold_gaussian\"],\n",
    "    \"seq_6\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\"],\n",
    "    \"seq_7\" : [\"median_blur\"],\n",
    "    \"seq_8\" : [\"gaussian_blur\"],\n",
    "    \"seq_9\" : [\"bilateral_blur\"],\n",
    "    \"seq_10\" : [\"fastnl_blur\"],\n",
    "    \"seq_11\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\", \"opening\"],\n",
    "    \"seq_12\" : [\"gray\", \"bilateral_blur\", \"threshold_otsu\", \"closing\"],\n",
    "    \"seq_13\" : [\"opening\"],\n",
    "    \"seq_14\" : [\"closing\"],\n",
    "    \"seq_15\" : [\"gray\", \"sobel\"],\n",
    "    \"seq_16\" : [\"gray\", \"laplacian\"],\n",
    "    \"seq_17\" : [\"gray\", \"canny\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config/Map Files Load\n",
    "* change below paths while running on GPU\n",
    "* you may prefer to dump config files/jsons in the same notebook dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary containing imagenet - custom label maps\n",
    "imagenet_label_map = None\n",
    "with open('./config_jsons/imagenet_label_map.json') as json_file: \n",
    "    imagenet_label_map = json.load(json_file)\n",
    "\n",
    "# dictionary containing model ids - model_name map\n",
    "model_ids = None\n",
    "with open('./config_jsons/model_ids_map.json') as json_file: \n",
    "    model_ids = json.load(json_file)\n",
    "\n",
    "# dictionary containing dataset_ids and dataset_desc    \n",
    "dataset_ids = None\n",
    "with open('../../dataset/image_classification/dataset_id_map.json') as json_file:\n",
    "    dataset_ids = json.load(json_file)\n",
    "    \n",
    "custom_dataset_labels = None\n",
    "with open('./config_jsons/label_ids_map.json') as json_file:\n",
    "    custom_dataset_labels = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider making below cell parameterised later using papermill, or you may prefer to do it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking path as a parameter here, to ease out refactoring in future.\n",
    "# first experimentation is on raw dataset, will scale out to others\n",
    "dataset_path = Path(\"../../dataset/image_classification/raw\")\n",
    "master_df_keras_path = \"./experiment_results/master_keras.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe columns\n",
    "columns = [\"model_id\", \"model_name\", \"seq_id\", \"seq_name\", \"image_name\", \"label_id\", \n",
    "             \"pred_label_id\", \"label_name\",\"pred_label_name\", \"pred_confidence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(columns = columns)\n",
    "# df.to_csv(master_df_keras_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_labels = [x for x in os.walk(dataset_path)][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_trained_on_299x299 = [\n",
    "    \"inception_v3\", \n",
    "    \"xception\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenCV to PIL image conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pil_img(opencv_img):\n",
    "    if opencv_img.dtype == 'float64':\n",
    "        opencv_img = opencv_img.astype(np.uint8)\n",
    "    pil_img = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(pil_img)\n",
    "    return pil_img\n",
    "\n",
    "def convert_to_cv_img(pil_img):\n",
    "    np_img_arr = np.asarray(pil_img)\n",
    "    cv_image=cv2.cvtColor(np_img_arr, cv2.COLOR_RGB2BGR)\n",
    "    return cv_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id(model_name):\n",
    "    for id, name in model_ids.items():\n",
    "        if name == model_name:\n",
    "            return id\n",
    "    return \"not found for model name: \" + model_name\n",
    "\n",
    "def get_model_name(id):\n",
    "    if id not in model_ids.keys():\n",
    "        return \"not found for model id: \" + str(id)\n",
    "    return model_ids[id]\n",
    "\n",
    "def get_seq_operations(seq_id):\n",
    "    return pre_processing_seq_dict[seq_id]\n",
    "\n",
    "def get_seq_name(seq_id):\n",
    "    if seq_id not in pre_processing_seq_dict.keys():\n",
    "        return \"not found for dataset id: \" + seq_id\n",
    "    return \" > \".join(get_seq_operations(seq_id))\n",
    "\n",
    "def get_label_id(label_name):\n",
    "    for id, name in custom_dataset_labels.items():\n",
    "        if name == label_name:\n",
    "            return id\n",
    "    return \"not found for label: \"+label_name\n",
    "\n",
    "def get_label_name(id):\n",
    "    if id not in custom_dataset_labels.keys():\n",
    "        return \"not found for id: \" + str(id)\n",
    "    return custom_dataset_labels[id]\n",
    "\n",
    "def get_pred_label_id(pred_label_name):\n",
    "    for custom_label, imagenet_labels in imagenet_label_map.items():\n",
    "        if pred_label_name.lower() in imagenet_labels:\n",
    "            return get_label_id(custom_label)\n",
    "    return get_label_id(\"others\")\n",
    "\n",
    "def load_master_df_from_csv():\n",
    "    return pd.read_csv(master_df_keras_path)\n",
    "\n",
    "def dump_master_df_to_csv(master_df):\n",
    "    master_df.to_csv(master_df_keras_path, header=True, columns=columns, index=False)       \n",
    "\n",
    "def log_per_label_results_to_csv(model_name, label_name, pred_result_list, seq_id):\n",
    "    # input - pred_result_list: (label, pred_tuple_list), pred_tuple_list: [(image_name, pred_label, pred_percentage)]\n",
    "    # columns = [\"model_id\", \"model_name\", \"dataset_id\", \"dataset_name\", \"image_name\", \"label_id\", \n",
    "    #            \"pred_label_id\", \"label_name\",\"pred_label_name\", \"pred_confidence\"]\n",
    "\n",
    "    model_id = get_model_id(model_name)\n",
    "    model_name = model_name\n",
    "    # change below before running on pre processed image dataset\n",
    "    seq_id = seq_id\n",
    "    seq_name = get_seq_name(seq_id)\n",
    "    label_id = get_label_id(label_name)\n",
    "    label_name = label_name\n",
    "    \n",
    "    rows = []\n",
    "    for res in pred_result_list:\n",
    "        image_name = res[0]\n",
    "        pred_label_name = res[1]\n",
    "        pred_confidence = res[2]\n",
    "        pred_label_id = get_pred_label_id(pred_label_name)\n",
    "        row = [model_id, model_name, seq_id, seq_name, image_name, label_id, label_name, \n",
    "               pred_label_id, pred_label_name, pred_confidence]\n",
    "        rows.append(row)\n",
    "    \n",
    "    current_label_df = pd.DataFrame(rows, columns = columns)\n",
    "    master_df = load_master_df_from_csv()\n",
    "    master_df = master_df.append(current_label_df)\n",
    "    dump_master_df_to_csv(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_per_label_imgs_generator(size_default, seq_id):\n",
    "    # better due to memory restrictions we read per class images at once i.e airplane folder at a time.\n",
    "    # output: (label, [(image_name, pil_image)])\n",
    "    for label in custom_labels:\n",
    "        per_label_images = []\n",
    "        label_image_names = [x for x in os.walk(dataset_path/label)][0][2]\n",
    "        for image_name in label_image_names:\n",
    "            img_path = dataset_path/label/image_name\n",
    "            if size_default:\n",
    "                img = image.load_img(img_path, color_mode=\"rgb\", target_size=(224, 224))\n",
    "            else:\n",
    "                img = image.load_img(img_path, color_mode=\"rgb\", target_size=(299, 299))\n",
    "            img = apply_cv_transformations(seq_id, img)\n",
    "            x = image.img_to_array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            per_label_images.append((image_name, x))\n",
    "        yield (label, per_label_images)\n",
    "\n",
    "def apply_cv_transformations(seq_id, pil_img):\n",
    "    cv_img = convert_to_cv_img(pil_img)\n",
    "    operations = get_seq_operations(seq_id)\n",
    "    processed_img = cv_img\n",
    "    for operation in operations:\n",
    "        processed_img = lib.dispatcher[operation](processed_img)\n",
    "    return convert_to_pil_img(processed_img) \n",
    "    \n",
    "        \n",
    "def transform_images(preprocess_method, imgs_tuple):\n",
    "    # input: imgs_tuple: (image_name, pil_image)\n",
    "    # output: processed_imgs: (image_name, processed_image)\n",
    "    processed_imgs = []\n",
    "    \n",
    "    for img_tuple in imgs_tuple:\n",
    "        img = preprocess_method(img_tuple[1])\n",
    "        processed_imgs.append((img_tuple[0],img))\n",
    "    return processed_imgs\n",
    "\n",
    "\n",
    "def evaluate_results(model, imgs, decode_predictions):\n",
    "    # input: imgs = (img_name, pil_image)\n",
    "    # output: results = [(img_name, pred_label, prob)]\n",
    "    results = []\n",
    "    for img in imgs:\n",
    "        pred = model.predict(img[1])\n",
    "        (id, label, label_prob) = decode_predictions(pred, top=1)[0][0]\n",
    "        results.append((img[0], label, label_prob))\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_per_label_model_inference(model, model_name, preprocess_method, decode_predictions, default_size, seq_id):\n",
    "    # input - model: which model?, preprocess_method: model dependent method, decode_predictions: model dependent\n",
    "    # input - default_size = true, means load image in 299x299 or 224x224?\n",
    "    # imgs_tuple: (label, [images])\n",
    "    # output - (label, pred_tuple_list), here pred_tuple_list: [(image_name, pred_label, prob)]\n",
    "    for imgs_tuple in load_per_label_imgs_generator(default_size, seq_id):\n",
    "        processed_imgs = transform_images(preprocess_method, imgs_tuple[1])\n",
    "        pred_res = evaluate_results(model, processed_imgs, decode_predictions)\n",
    "        label_name = imgs_tuple[0]\n",
    "        log_per_label_results_to_csv(model_name, label_name, pred_res, seq_id)\n",
    "    \n",
    "def model_generator():\n",
    "    for model_name in ptms.keys():\n",
    "        yield (model_name, ptms[model_name])    \n",
    "\n",
    "def run_all_model_inference():\n",
    "    for model_tuple in model_generator():\n",
    "        #model_tuple = next(model_generator())\n",
    "        model_name = model_tuple[0]\n",
    "        model = model_tuple[1]()\n",
    "        default_size = True\n",
    "        if model_name in models_trained_on_299x299:\n",
    "            default_size = False\n",
    "        print(\"Inference started for model: \"+model_name)\n",
    "        for seq_id in pre_processing_seq_dict.keys():\n",
    "            print(\"inference started for seq_id: \" + seq_id + \", \", end='')\n",
    "            run_per_label_model_inference(model[0], model_name, model[1], model[2], default_size, seq_id)\n",
    "            print(\"completed!\")\n",
    "        print(\"Inference completed! -----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Inference started for model: resnet50\n",
      "inference started for seq_id: seq_0, completed!\n",
      "inference started for seq_id: seq_1, completed!\n",
      "Inference completed! -----------------------------------\n",
      "Inference started for model: vgg19\n",
      "inference started for seq_id: seq_0, completed!\n",
      "inference started for seq_id: seq_1, completed!\n",
      "Inference completed! -----------------------------------\n",
      "Inference started for model: xception\n",
      "inference started for seq_id: seq_0, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-14-6433139424a4>\", line 2, in <module>\n",
      "    run_all_model_inference()\n",
      "  File \"<ipython-input-13-ff3e0af51c62>\", line 76, in run_all_model_inference\n",
      "    run_per_label_model_inference(model[0], model_name, model[1], model[2], default_size, seq_id)\n",
      "  File \"<ipython-input-13-ff3e0af51c62>\", line 57, in run_per_label_model_inference\n",
      "    pred_res = evaluate_results(model, processed_imgs, decode_predictions)\n",
      "  File \"<ipython-input-13-ff3e0af51c62>\", line 44, in evaluate_results\n",
      "    pred = model.predict(img[1])\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\", line 1401, in predict\n",
      "    callbacks=callbacks)\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training_arrays.py\", line 332, in predict_loop\n",
      "    batch_outs = f(ins_batch)\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2979, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2937, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\contrib\\__init__.py\", line 47, in <module>\n",
      "    from tensorflow.contrib import distributions\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\contrib\\distributions\\__init__.py\", line 29, in <module>\n",
      "    from tensorflow.contrib.distributions.python.ops import bijectors\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\contrib\\distributions\\__init__.py\", line 44, in <module>\n",
      "    from tensorflow.contrib.distributions.python.ops.estimator import *\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\contrib\\distributions\\python\\ops\\estimator.py\", line 21, in <module>\n",
      "    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\contrib\\learn\\__init__.py\", line 93, in <module>\n",
      "    from tensorflow.contrib.learn.python.learn import *\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\contrib\\learn\\python\\__init__.py\", line 28, in <module>\n",
      "    from tensorflow.contrib.learn.python.learn import *\n",
      "  File \"C:\\Users\\shubham\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\__init__.py\", line 30, in <module>\n",
      "    from tensorflow.contrib.learn.python.learn import estimators\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# need to change the save master_df path and assign model ids as well\n",
    "run_all_model_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
